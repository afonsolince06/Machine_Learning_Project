{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc993f4",
   "metadata": {},
   "source": [
    "# **Notebook 4**\n",
    "## **Modelling and Tuning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd5e20",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook marks the official transition to the core Machine Learning phase. Our primary objective is to engage in rigorous model evaluation and refinement to identify the best classification algorithm for predicting the binary quality of the PastÃ©is de Nata.  \n",
    "\n",
    "This involves a strict, comparative analysis using the clean, anti-leakage data partitions (Train and Validation) prepared in Notebook 3.   \n",
    "We will follow this steps:  \n",
    "- **Establish Baseline Performance:** We will train a diverse portfolio of models using default settings to establish a baseline performance and potential.\n",
    "- **Diagnose Overfitting:** By comparing performance metrics across the Training and Validation sets, we will precisely diagnose model generalization ability versus **overfitting**.\n",
    "- **Systematic Optimization:** We will select the most promising models and optimize their complexity and performance using **GridSearchCV**  combined with the robust **Stratified K-Fold Cross-Validation (SKF)**  loaded from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85694c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93695220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62962de6",
   "metadata": {},
   "source": [
    "##### **4.1 Load transformed data and partitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090ed57",
   "metadata": {},
   "source": [
    "Loads the master dictionary saved in Notebook 3, which contains all pre-processed and standardized data partitions (`X_train`, `X_val`, `X_test`) and their corresponding target variables (`y_train`, `y_val`, `y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a62522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load train/val/test split data from notebook3\n",
    "with open(r'Nata_Files\\\\train_test_split_fixed.pkl', 'rb') as f:\n",
    "    notebook3_data = pickle.load(f)\n",
    "\n",
    "\n",
    "X_train = notebook3_data['X_train']\n",
    "X_val = notebook3_data['X_val']\n",
    "X_test = notebook3_data['X_test']\n",
    "y_train = notebook3_data['y_train']\n",
    "y_val = notebook3_data['y_val']\n",
    "y_test = notebook3_data['y_test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac941e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c08a8f",
   "metadata": {},
   "source": [
    "These three functions serve to **standardize and centralize** the fundamental operations required for every classification model: training the model (`fit`), extracting continuous scores (`predict_proba`), and obtaining the final binary prediction (`predict`), ensuring uniformity in the evaluation code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac7da53",
   "metadata": {},
   "source": [
    "#### **4.2 Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6183c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1981, number of negative: 1138\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1722\n",
      "[LightGBM] [Info] Number of data points in the train set: 3119, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.635139 -> initscore=0.554329\n",
      "[LightGBM] [Info] Start training from score 0.554329\n"
     ]
    }
   ],
   "source": [
    "logr = LogisticRegression(random_state=42)\n",
    "logr.fit(X_train, y_train)\n",
    "logr_proba = logr.predict_proba(X_val)[:,1]\n",
    "logr_pred = logr.predict(X_val) \n",
    "logr_proba_tr = logr.predict_proba(X_train)[:,1]\n",
    "logr_pred_tr = logr.predict(X_train)\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "dtc.fit(X_train, y_train)\n",
    "dtc_proba = dtc.predict_proba(X_val)[:,1]\n",
    "dtc_pred = dtc.predict(X_val)\n",
    "dtc_proba_tr = dtc.predict_proba(X_train)[:,1]\n",
    "dtc_pred_tr = dtc.predict(X_train)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_proba = rf.predict_proba(X_val)[:,1]\n",
    "rf_pred = rf.predict(X_val)\n",
    "rf_proba_tr = rf.predict_proba(X_train)[:,1]\n",
    "rf_pred_tr = rf.predict(X_train)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn_proba = knn.predict_proba(X_val)[:,1]\n",
    "knn_pred = knn.predict(X_val)\n",
    "knn_proba_tr = knn.predict_proba(X_train)[:,1]\n",
    "knn_pred_tr = knn.predict(X_train)\n",
    "\n",
    "lgb = LGBMClassifier(random_state=42)\n",
    "lgb.fit(X_train, y_train)\n",
    "lgb_proba = lgb.predict_proba(X_val)[:,1]\n",
    "lgb_pred = lgb.predict(X_val)\n",
    "lgb_proba_tr = lgb.predict_proba(X_train)[:,1]\n",
    "lgb_pred_tr = lgb.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9a0f28",
   "metadata": {},
   "source": [
    "#### **4.3 Define the metrics function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e670b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_val, y_proba, y_pred, model, dataset):\n",
    "    return {\n",
    "        \"Model\" : model,\n",
    "        \"Set\" : dataset,\n",
    "        \"AUC\": roc_auc_score(y_val, y_proba),\n",
    "        \"Accuracy\": accuracy_score(y_val, y_pred),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594dc32f",
   "metadata": {},
   "source": [
    "Inputs: It takes the true target values (`y_val`), the predicted probabilities (`y_proba`), the final predicted classes (`y_pred`), the model name, and the dataset name (\"Train\" or \"Validation\").  \n",
    "Output: It returns a dictionary containing the **AUC** (Area Under the Curve) and **Accuracy** metrics. AUC requires the probability scores (`y_proba`), while Accuracy requires the binary class predictions (`y_pred`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450da354",
   "metadata": {},
   "source": [
    "#### **4.4 Model Evaluatiion and Metrics collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138674f6",
   "metadata": {},
   "source": [
    "This crucial step executes the evaluation function (`get_metrics`) for all five baseline models across both the Training and Validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78f5d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_metrics = []\n",
    "\n",
    "models_metrics.append(get_metrics(y_train, logr_proba_tr, logr_pred_tr, \"Logistic Regression\", \"Train\"))\n",
    "models_metrics.append(get_metrics(y_train, dtc_proba_tr, dtc_pred_tr, \"DTClassifier\", \"Train\"))\n",
    "models_metrics.append(get_metrics(y_train, rf_proba_tr, rf_pred_tr, \"Random Forest\", \"Train\"))\n",
    "models_metrics.append(get_metrics(y_train, knn_proba_tr, knn_pred_tr, \"KNClassifier\", \"Train\"))\n",
    "models_metrics.append(get_metrics(y_train, lgb_proba_tr, lgb_pred_tr, \"LightGBM\", \"Train\"))\n",
    "\n",
    "models_metrics.append(get_metrics(y_val, logr_proba, logr_pred, \"Logistic Regression\", \"Validation\"))\n",
    "models_metrics.append(get_metrics(y_val, dtc_proba, dtc_pred, \"DTClassifier\", \"Validation\"))\n",
    "models_metrics.append(get_metrics(y_val, rf_proba, rf_pred, \"Random Forest\", \"Validation\"))\n",
    "models_metrics.append(get_metrics(y_val, knn_proba, knn_pred, \"KNClassifier\", \"Validation\"))\n",
    "models_metrics.append(get_metrics(y_val, lgb_proba, lgb_pred, \"LightGBM\", \"Validation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7721904",
   "metadata": {},
   "source": [
    "The metrics are first collected on the training set to measure the model's fiting ability. The metrics are then collected on the validation set to measure the model's generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fb222e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th>Set</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">DTClassifier</th>\n",
       "      <th>Train</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.665391</td>\n",
       "      <td>0.686538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">KNClassifier</th>\n",
       "      <th>Train</th>\n",
       "      <td>0.894342</td>\n",
       "      <td>0.819493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.757484</td>\n",
       "      <td>0.701923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LightGBM</th>\n",
       "      <th>Train</th>\n",
       "      <td>0.997112</td>\n",
       "      <td>0.972748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.799442</td>\n",
       "      <td>0.742308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Logistic Regression</th>\n",
       "      <th>Train</th>\n",
       "      <td>0.800070</td>\n",
       "      <td>0.741263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.790423</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Random Forest</th>\n",
       "      <th>Train</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.817711</td>\n",
       "      <td>0.759615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     AUC  Accuracy\n",
       "Model               Set                           \n",
       "DTClassifier        Train       1.000000  1.000000\n",
       "                    Validation  0.665391  0.686538\n",
       "KNClassifier        Train       0.894342  0.819493\n",
       "                    Validation  0.757484  0.701923\n",
       "LightGBM            Train       0.997112  0.972748\n",
       "                    Validation  0.799442  0.742308\n",
       "Logistic Regression Train       0.800070  0.741263\n",
       "                    Validation  0.790423  0.737500\n",
       "Random Forest       Train       1.000000  1.000000\n",
       "                    Validation  0.817711  0.759615"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models_metrics = pd.DataFrame(models_metrics)\n",
    "df_models_metrics = df_models_metrics.pivot_table(index=[\"Model\", \"Set\"], values=[\"AUC\", \"Accuracy\"])\n",
    "\n",
    "df_models_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0844e8e",
   "metadata": {},
   "source": [
    "## Choosing the best model\n",
    "- We decided to use Random Forest and LightGBM as our baseline models and Logistic Regression as the metamodel on Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896fd7ed",
   "metadata": {},
   "source": [
    "### **LightGBM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c68ecae",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1981, number of negative: 1138\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1722\n",
      "[LightGBM] [Info] Number of data points in the train set: 3119, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.635139 -> initscore=0.554329\n",
      "[LightGBM] [Info] Start training from score 0.554329\n",
      "Best params: {'subsample': 0.8, 'num_leaves': 15, 'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.1, 'colsample_bytree': 1.0}  Best CV AUC: 0.825499972981131\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'num_leaves': [15, 31, 63],\n",
    "    'max_depth': [5, 10, -1], # -1 means no limit\n",
    "    'learning_rate': [0.1, 0.03, 0.01],\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]}\n",
    "\n",
    "lgb_clf = LGBMClassifier(random_state=42)\n",
    "rs = RandomizedSearchCV(lgb_clf, param_dist, n_iter=20, cv=3,\n",
    "                        scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "rs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", rs.best_params_)\n",
    "best_lgb = rs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69600eb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
