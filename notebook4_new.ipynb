{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc993f4",
   "metadata": {},
   "source": [
    "# **Notebook 4**\n",
    "## **Modelling and Tuning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd5e20",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook marks the official transition to the core Machine Learning phase. Our primary objective is to engage in rigorous model evaluation and refinement to identify the best classification algorithm for predicting the binary quality of the PastÃ©is de Nata.  \n",
    "\n",
    "This involves a strict, comparative analysis using the clean, anti-leakage data partitions (Train and Validation) prepared in Notebook 3.   \n",
    "We will follow this steps:  \n",
    "- **Establish Baseline Performance:** We will train a diverse portfolio of models using default settings to establish a baseline performance and potential.\n",
    "- **Diagnose Overfitting:** By comparing performance metrics across the Training and Validation sets, we will precisely diagnose model generalization ability versus **overfitting**.\n",
    "- **Systematic Optimization:** We will select the most promising models and optimize their complexity and performance using **GridSearchCV**  combined with the robust **Stratified K-Fold Cross-Validation (SKF)**  loaded from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85694c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93695220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62962de6",
   "metadata": {},
   "source": [
    "##### **4.1 Load transformed data and partitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090ed57",
   "metadata": {},
   "source": [
    "Loads the master dictionary saved in Notebook 3, which contains all pre-processed and standardized data partitions (`X_train`, `X_val`, `X_test`) and their corresponding target variables (`y_train`, `y_val`, `y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35a62522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load train/val/test split data from notebook3\n",
    "with open(r'Nata_Files\\\\train_test_split_fixed.pkl', 'rb') as f:\n",
    "    notebook3_data = pickle.load(f)\n",
    "\n",
    "\n",
    "X_train = notebook3_data['X_train']\n",
    "X_val = notebook3_data['X_val']\n",
    "X_test = notebook3_data['X_test']\n",
    "y_train = notebook3_data['y_train']\n",
    "y_val = notebook3_data['y_val']\n",
    "y_test = notebook3_data['y_test']\n",
    "X_predict = notebook3_data['X_predict_final']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac941e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c08a8f",
   "metadata": {},
   "source": [
    "These three functions serve to **standardize and centralize** the fundamental operations required for every classification model: training the model (`fit`), extracting continuous scores (`predict_proba`), and obtaining the final binary prediction (`predict`), ensuring uniformity in the evaluation code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac7da53",
   "metadata": {},
   "source": [
    "#### **4.2 Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6183c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1981, number of negative: 1138\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1722\n",
      "[LightGBM] [Info] Number of data points in the train set: 3119, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.635139 -> initscore=0.554329\n",
      "[LightGBM] [Info] Start training from score 0.554329\n"
     ]
    }
   ],
   "source": [
    "logr = LogisticRegression(random_state=42)\n",
    "logr.fit(X_train, y_train)\n",
    "logr_proba = logr.predict_proba(X_val)[:,1]\n",
    "logr_pred = logr.predict(X_val) \n",
    "logr_proba_tr = logr.predict_proba(X_train)[:,1]\n",
    "logr_pred_tr = logr.predict(X_train)\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "dtc.fit(X_train, y_train)\n",
    "dtc_proba = dtc.predict_proba(X_val)[:,1]\n",
    "dtc_pred = dtc.predict(X_val)\n",
    "dtc_proba_tr = dtc.predict_proba(X_train)[:,1]\n",
    "dtc_pred_tr = dtc.predict(X_train)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_proba = rf.predict_proba(X_val)[:,1]\n",
    "rf_pred = rf.predict(X_val)\n",
    "rf_proba_tr = rf.predict_proba(X_train)[:,1]\n",
    "rf_pred_tr = rf.predict(X_train)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn_proba = knn.predict_proba(X_val)[:,1]\n",
    "knn_pred = knn.predict(X_val)\n",
    "knn_proba_tr = knn.predict_proba(X_train)[:,1]\n",
    "knn_pred_tr = knn.predict(X_train)\n",
    "\n",
    "lgb = LGBMClassifier(random_state=42)\n",
    "lgb.fit(X_train, y_train)\n",
    "lgb_proba = lgb.predict_proba(X_val)[:,1]\n",
    "lgb_pred = lgb.predict(X_val)\n",
    "lgb_proba_tr = lgb.predict_proba(X_train)[:,1]\n",
    "lgb_pred_tr = lgb.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9a0f28",
   "metadata": {},
   "source": [
    "#### **4.3 Define the metrics function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e670b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_val, y_proba, y_pred, model, dataset):\n",
    "    return {\n",
    "        \"Model\" : model,\n",
    "        \"Set\" : dataset,\n",
    "        \"AUC\": roc_auc_score(y_val, y_proba),\n",
    "        \"Accuracy\": accuracy_score(y_val, y_pred),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594dc32f",
   "metadata": {},
   "source": [
    "Inputs: It takes the true target values (`y_val`), the predicted probabilities (`y_proba`), the final predicted classes (`y_pred`), the model name, and the dataset name (\"Train\" or \"Validation\").  \n",
    "Output: It returns a dictionary containing the **AUC** (Area Under the Curve) and **Accuracy** metrics. AUC requires the probability scores (`y_proba`), while Accuracy requires the binary class predictions (`y_pred`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450da354",
   "metadata": {},
   "source": [
    "#### **4.4 Model Evaluatiion and Metrics collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138674f6",
   "metadata": {},
   "source": [
    "This crucial step executes the evaluation function (`get_metrics`) for all five baseline models across both the Training and Validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78f5d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_metrics = []\n",
    "\n",
    "models_metrics.append(get_metrics(y_train, logr_proba_tr, logr_pred_tr, \"Logistic Regression\", \"Train\"))\n",
    "models_metrics.append(get_metrics(y_train, dtc_proba_tr, dtc_pred_tr, \"DTClassifier\", \"Train\"))\n",
    "models_metrics.append(get_metrics(y_train, rf_proba_tr, rf_pred_tr, \"Random Forest\", \"Train\"))\n",
    "models_metrics.append(get_metrics(y_train, knn_proba_tr, knn_pred_tr, \"KNClassifier\", \"Train\"))\n",
    "models_metrics.append(get_metrics(y_train, lgb_proba_tr, lgb_pred_tr, \"LightGBM\", \"Train\"))\n",
    "\n",
    "models_metrics.append(get_metrics(y_val, logr_proba, logr_pred, \"Logistic Regression\", \"Validation\"))\n",
    "models_metrics.append(get_metrics(y_val, dtc_proba, dtc_pred, \"DTClassifier\", \"Validation\"))\n",
    "models_metrics.append(get_metrics(y_val, rf_proba, rf_pred, \"Random Forest\", \"Validation\"))\n",
    "models_metrics.append(get_metrics(y_val, knn_proba, knn_pred, \"KNClassifier\", \"Validation\"))\n",
    "models_metrics.append(get_metrics(y_val, lgb_proba, lgb_pred, \"LightGBM\", \"Validation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7721904",
   "metadata": {},
   "source": [
    "The metrics are first collected on the training set to measure the model's fiting ability. The metrics are then collected on the validation set to measure the model's generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fb222e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th>Set</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">DTClassifier</th>\n",
       "      <th>Train</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.665391</td>\n",
       "      <td>0.686538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">KNClassifier</th>\n",
       "      <th>Train</th>\n",
       "      <td>0.894342</td>\n",
       "      <td>0.819493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.757484</td>\n",
       "      <td>0.701923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LightGBM</th>\n",
       "      <th>Train</th>\n",
       "      <td>0.997112</td>\n",
       "      <td>0.972748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.799442</td>\n",
       "      <td>0.742308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Logistic Regression</th>\n",
       "      <th>Train</th>\n",
       "      <td>0.800070</td>\n",
       "      <td>0.741263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.790423</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Random Forest</th>\n",
       "      <th>Train</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.817711</td>\n",
       "      <td>0.759615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     AUC  Accuracy\n",
       "Model               Set                           \n",
       "DTClassifier        Train       1.000000  1.000000\n",
       "                    Validation  0.665391  0.686538\n",
       "KNClassifier        Train       0.894342  0.819493\n",
       "                    Validation  0.757484  0.701923\n",
       "LightGBM            Train       0.997112  0.972748\n",
       "                    Validation  0.799442  0.742308\n",
       "Logistic Regression Train       0.800070  0.741263\n",
       "                    Validation  0.790423  0.737500\n",
       "Random Forest       Train       1.000000  1.000000\n",
       "                    Validation  0.817711  0.759615"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models_metrics = pd.DataFrame(models_metrics)\n",
    "df_models_metrics = df_models_metrics.pivot_table(index=[\"Model\", \"Set\"], values=[\"AUC\", \"Accuracy\"])\n",
    "\n",
    "df_models_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0844e8e",
   "metadata": {},
   "source": [
    "## Choosing the best model\n",
    "- We decided to use Random Forest and LightGBM as our baseline models and Logistic Regression as the metamodel on Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896fd7ed",
   "metadata": {},
   "source": [
    "### **LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2593518",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = LGBMClassifier(random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c68ecae",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eb0dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1981, number of negative: 1138\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1722\n",
      "[LightGBM] [Info] Number of data points in the train set: 3119, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.635139 -> initscore=0.554329\n",
      "[LightGBM] [Info] Start training from score 0.554329\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best params: {'subsample': 1.0, 'num_leaves': 63, 'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "param_space = {\n",
    "    'num_leaves': [15, 31, 63],\n",
    "    'max_depth': [5, 10, -1], # -1 means no limit\n",
    "    'learning_rate': [0.1, 0.03, 0.01],\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]}\n",
    "\n",
    "lgb_clf_rs = RandomizedSearchCV(lgb_clf, param_space, n_iter=20, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "lgb_clf_rs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", lgb_clf_rs.best_params_)\n",
    "best_lgb = lgb_clf_rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "418fdf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.808, Accuracy: 0.747\n"
     ]
    }
   ],
   "source": [
    "best_lgb_proba = best_lgb.predict_proba(X_val)[:,1]\n",
    "best_lgb_pred = best_lgb.predict(X_val)\n",
    "\n",
    "print(f\"AUC: {roc_auc_score(y_val, best_lgb_proba):.3f}, Accuracy: {accuracy_score(y_val, best_lgb_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69600eb",
   "metadata": {},
   "source": [
    "### **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f43bebe",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f2fae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "058a0c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tiago\\anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "21 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "21 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tiago\\anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tiago\\anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\tiago\\anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\tiago\\anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_depth' parameter of RandomForestClassifier must be an int in the range [1, inf) or None. Got -1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\tiago\\anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [       nan 0.76819396        nan 0.7678753  0.75119814 0.7707593\n",
      "        nan 0.77108043 0.75055959        nan        nan 0.77364515\n",
      "        nan 0.7508801  0.75151927 0.76883653 0.75151958        nan\n",
      " 0.74703148 0.76755139]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 10, 'criterion': 'entropy', 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "param_range = {\n",
    "    'n_estimators': [100, 200, 400, 600],\n",
    "    'max_depth': [5, 10, -1], # -1 means no limit\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_clf_rs = RandomizedSearchCV(rf_clf, param_range, n_iter=20, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "rf_clf_rs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", rf_clf_rs.best_params_)\n",
    "best_rf = rf_clf_rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf52b343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.814, Accuracy: 0.769\n"
     ]
    }
   ],
   "source": [
    "best_rf_proba = best_rf.predict_proba(X_val)[:,1]\n",
    "best_rf_pred = best_rf.predict(X_val)\n",
    "\n",
    "print(f\"AUC: {roc_auc_score(y_val, best_rf_proba):.3f}, Accuracy: {accuracy_score(y_val, best_rf_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014fde3d",
   "metadata": {},
   "source": [
    "## **KNClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c3d25fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa2949",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "365135ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'weights': 'distance', 'n_neighbors': 7, 'metric': 'manhattan', 'leaf_size': 30}\n"
     ]
    }
   ],
   "source": [
    "param_set = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'leaf_size': [20, 30, 40, 50],\n",
    "    'metric': ['minkowski', 'euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "knn_clf_rs = RandomizedSearchCV(knn_clf, param_set, n_iter=20, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "knn_clf_rs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", knn_clf_rs.best_params_)\n",
    "best_knn = knn_clf_rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3c32fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.786, Accuracy: 0.730\n"
     ]
    }
   ],
   "source": [
    "best_knn_proba = best_knn.predict_proba(X_val)[:,1]\n",
    "best_knn_pred = best_knn.predict(X_val)\n",
    "\n",
    "print(f\"AUC: {roc_auc_score(y_val, best_knn_proba):.3f}, Accuracy: {accuracy_score(y_val, best_knn_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2061c108",
   "metadata": {},
   "source": [
    "## **Stacking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92dd0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('rf', best_rf),\n",
    "    ('lgb', best_lgb),\n",
    "    ('knn', best_knn)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c284c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1981, number of negative: 1138\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1722\n",
      "[LightGBM] [Info] Number of data points in the train set: 3119, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.635139 -> initscore=0.554329\n",
      "[LightGBM] [Info] Start training from score 0.554329\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 1584, number of negative: 911\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1695\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.634870 -> initscore=0.553166\n",
      "[LightGBM] [Info] Start training from score 0.553166\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 1585, number of negative: 910\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1691\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.635271 -> initscore=0.554895\n",
      "[LightGBM] [Info] Start training from score 0.554895\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 1585, number of negative: 910\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1693\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.635271 -> initscore=0.554895\n",
      "[LightGBM] [Info] Start training from score 0.554895\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 1585, number of negative: 910\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1696\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.635271 -> initscore=0.554895\n",
      "[LightGBM] [Info] Start training from score 0.554895\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 1585, number of negative: 911\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1694\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.635016 -> initscore=0.553797\n",
      "[LightGBM] [Info] Start training from score 0.553797\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "stc = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression()).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "718721b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.819, Accuracy: 0.767\n"
     ]
    }
   ],
   "source": [
    "stc_proba = stc.predict_proba(X_val)[:,1]\n",
    "stc_pred = stc.predict(X_val)\n",
    "\n",
    "print(f\"AUC: {roc_auc_score(y_val, stc_proba):.3f}, Accuracy: {accuracy_score(y_val, stc_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3901858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_pred_predict = stc.predict(X_predict)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id':X_predict.index,\n",
    "    'Quality_class': stc_pred_predict\n",
    "})\n",
    "submission_df['Quality_class'] = submission_df['Quality_class'].map({0: 'KO', 1: 'OK'})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af54891a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
