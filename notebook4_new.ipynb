{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc993f4",
   "metadata": {},
   "source": [
    "# **Notebook 4**\n",
    "## **Modelling and Tuning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd5e20",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook marks the official transition to the core Machine Learning phase. Our primary objective is to engage in rigorous model evaluation and refinement to identify the best classification algorithm for predicting the binary quality of the Pastéis de Nata.  \n",
    "\n",
    "This involves a strict, comparative analysis using the clean, anti-leakage data partitions (Train and Validation) prepared in Notebook 3.   \n",
    "We will follow this steps:  \n",
    "- **Establish Baseline Performance:** We will train a diverse portfolio of models using default settings to establish a baseline performance and potential.\n",
    "- **Diagnose Overfitting:** By comparing performance metrics across the Training and Validation sets, we will precisely diagnose model generalization ability versus **overfitting**.\n",
    "- **Systematic Optimization:** We will select the most promising models and optimize their complexity and performance using **GridSearchCV**  combined with the robust **Stratified K-Fold Cross-Validation (SKF)**  loaded from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85694c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93695220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62962de6",
   "metadata": {},
   "source": [
    "##### **4.1 Load transformed data and partitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090ed57",
   "metadata": {},
   "source": [
    "Loads the master dictionary saved in Notebook 3, which contains all pre-processed and standardized data partitions (`X_train`, `X_val`, `X_test`) and their corresponding target variables (`y_train`, `y_val`, `y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "35a62522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load train/val/test split data from notebook3\n",
    "with open(r'Nata_Files\\\\train_test_split_fixed.pkl', 'rb') as f:\n",
    "    notebook3_data = pickle.load(f)\n",
    "\n",
    "\n",
    "X_train = notebook3_data['X_train']\n",
    "X_val = notebook3_data['X_val']\n",
    "X_test = notebook3_data['X_test']\n",
    "y_train = notebook3_data['y_train']\n",
    "y_val = notebook3_data['y_val']\n",
    "y_test = notebook3_data['y_test']\n",
    "X_predict = notebook3_data['X_predict_final']\n",
    "id_predict = notebook3_data['id_predict']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac941e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c08a8f",
   "metadata": {},
   "source": [
    "These three functions serve to **standardize and centralize** the fundamental operations required for every classification model: training the model (`fit`), extracting continuous scores (`predict_proba`), and obtaining the final binary prediction (`predict`), ensuring uniformity in the evaluation code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac7da53",
   "metadata": {},
   "source": [
    "#### **4.2 Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6183c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2311, number of negative: 1328\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2221\n",
      "[LightGBM] [Info] Number of data points in the train set: 3639, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.635065 -> initscore=0.554006\n",
      "[LightGBM] [Info] Start training from score 0.554006\n"
     ]
    }
   ],
   "source": [
    "logr = LogisticRegression(random_state=42)\n",
    "logr.fit(X_train, y_train)\n",
    "logr_proba = logr.predict_proba(X_val)[:,1]\n",
    "logr_pred = logr.predict(X_val) \n",
    "logr_proba_tr = logr.predict_proba(X_train)[:,1]\n",
    "logr_pred_tr = logr.predict(X_train)\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "dtc.fit(X_train, y_train)\n",
    "dtc_proba = dtc.predict_proba(X_val)[:,1]\n",
    "dtc_pred = dtc.predict(X_val)\n",
    "dtc_proba_tr = dtc.predict_proba(X_train)[:,1]\n",
    "dtc_pred_tr = dtc.predict(X_train)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_proba = rf.predict_proba(X_val)[:,1]\n",
    "rf_pred = rf.predict(X_val)\n",
    "rf_proba_tr = rf.predict_proba(X_train)[:,1]\n",
    "rf_pred_tr = rf.predict(X_train)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn_proba = knn.predict_proba(X_val)[:,1]\n",
    "knn_pred = knn.predict(X_val)\n",
    "knn_proba_tr = knn.predict_proba(X_train)[:,1]\n",
    "knn_pred_tr = knn.predict(X_train)\n",
    "\n",
    "lgb = LGBMClassifier(random_state=42)\n",
    "lgb.fit(X_train, y_train)\n",
    "lgb_proba = lgb.predict_proba(X_val)[:,1]\n",
    "lgb_pred = lgb.predict(X_val)\n",
    "lgb_proba_tr = lgb.predict_proba(X_train)[:,1]\n",
    "lgb_pred_tr = lgb.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9a0f28",
   "metadata": {},
   "source": [
    "#### **4.3 Define the metrics function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e670b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_val, y_proba, y_pred, model, dataset):\n",
    "    return {\n",
    "        \"Model\" : model,\n",
    "        \"Set\" : dataset,\n",
    "        \"Accuracy\": accuracy_score(y_val, y_pred),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594dc32f",
   "metadata": {},
   "source": [
    "Inputs: It takes the true target values (`y_val`), the predicted probabilities (`y_proba`), the final predicted classes (`y_pred`), the model name, and the dataset name (\"Train\" or \"Validation\").  \n",
    "Output: It returns a dictionary containing the **AUC** (Area Under the Curve) and **Accuracy** metrics. AUC requires the probability scores (`y_proba`), while Accuracy requires the binary class predictions (`y_pred`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450da354",
   "metadata": {},
   "source": [
    "#### **4.4 Model Evaluatiion and Metrics collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138674f6",
   "metadata": {},
   "source": [
    "This crucial step executes the evaluation function (`get_metrics`) for all five baseline models across both the Training and Validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "78f5d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_metrics = []\n",
    "\n",
    "models_metrics.append(get_metrics(y_train, logr_proba_tr, logr_pred_tr, \"Logistic Regression\", \"Train\"))\n",
    "models_metrics.append(get_metrics(y_train, dtc_proba_tr, dtc_pred_tr, \"DTClassifier\", \"Train\"))\n",
    "models_metrics.append(get_metrics(y_train, rf_proba_tr, rf_pred_tr, \"Random Forest\", \"Train\"))\n",
    "models_metrics.append(get_metrics(y_train, knn_proba_tr, knn_pred_tr, \"KNClassifier\", \"Train\"))\n",
    "models_metrics.append(get_metrics(y_train, lgb_proba_tr, lgb_pred_tr, \"LightGBM\", \"Train\"))\n",
    "\n",
    "models_metrics.append(get_metrics(y_val, logr_proba, logr_pred, \"Logistic Regression\", \"Validation\"))\n",
    "models_metrics.append(get_metrics(y_val, dtc_proba, dtc_pred, \"DTClassifier\", \"Validation\"))\n",
    "models_metrics.append(get_metrics(y_val, rf_proba, rf_pred, \"Random Forest\", \"Validation\"))\n",
    "models_metrics.append(get_metrics(y_val, knn_proba, knn_pred, \"KNClassifier\", \"Validation\"))\n",
    "models_metrics.append(get_metrics(y_val, lgb_proba, lgb_pred, \"LightGBM\", \"Validation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7721904",
   "metadata": {},
   "source": [
    "The metrics are first collected on the training set to measure the model's fiting ability. The metrics are then collected on the validation set to measure the model's generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7fb222e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th>Set</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">DTClassifier</th>\n",
       "      <th>Train</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.689744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">KNClassifier</th>\n",
       "      <th>Train</th>\n",
       "      <td>0.829074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.755128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LightGBM</th>\n",
       "      <th>Train</th>\n",
       "      <td>0.956032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.780769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Logistic Regression</th>\n",
       "      <th>Train</th>\n",
       "      <td>0.752130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.732051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Random Forest</th>\n",
       "      <th>Train</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.794872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Accuracy\n",
       "Model               Set                 \n",
       "DTClassifier        Train       1.000000\n",
       "                    Validation  0.689744\n",
       "KNClassifier        Train       0.829074\n",
       "                    Validation  0.755128\n",
       "LightGBM            Train       0.956032\n",
       "                    Validation  0.780769\n",
       "Logistic Regression Train       0.752130\n",
       "                    Validation  0.732051\n",
       "Random Forest       Train       1.000000\n",
       "                    Validation  0.794872"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models_metrics = pd.DataFrame(models_metrics)\n",
    "df_models_metrics = df_models_metrics.pivot_table(index=[\"Model\", \"Set\"], values=[\"Accuracy\"])\n",
    "\n",
    "df_models_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0844e8e",
   "metadata": {},
   "source": [
    "## Choosing the best model\n",
    "- We decided to use Random Forest and LightGBM as our baseline models and Logistic Regression as the metamodel on Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "757a5be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896fd7ed",
   "metadata": {},
   "source": [
    "### **LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2593518",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = LGBMClassifier(random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c68ecae",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7eb0dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2311, number of negative: 1328\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000938 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2221\n",
      "[LightGBM] [Info] Number of data points in the train set: 3639, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.635065 -> initscore=0.554006\n",
      "[LightGBM] [Info] Start training from score 0.554006\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best params: {'reg_lambda': 1, 'num_leaves': 15, 'n_estimators': 300, 'min_child_samples': 30, 'max_depth': 5, 'learning_rate': 0.03}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_space = {\n",
    "    'num_leaves': [10, 15, 20],\n",
    "    'max_depth': [5, 7, 10], \n",
    "    'learning_rate': [0.1, 0.03, 0.01],\n",
    "    'n_estimators': [200, 300],\n",
    "    'min_child_samples': [20, 30, 40],\n",
    "    'reg_lambda': [0.1, 1, 10],\n",
    "}\n",
    "lgb_clf_rs = RandomizedSearchCV(lgb_clf, param_space, n_iter=20, cv= cv_strategy, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "lgb_clf_rs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", lgb_clf_rs.best_params_)\n",
    "best_lgb = lgb_clf_rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "48725844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.764\n",
      "Accuracy on train: 0.851\n"
     ]
    }
   ],
   "source": [
    "best_lgb_proba = best_lgb.predict_proba(X_val)[:,1]\n",
    "best_lgb_pred = best_lgb.predict(X_val)\n",
    "\n",
    "best_lgb_pred_tr = best_lgb.predict(X_train)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_val, best_lgb_pred):.3f}\")\n",
    "print(f\"Accuracy on train: {accuracy_score(y_train, best_lgb_pred_tr):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69600eb",
   "metadata": {},
   "source": [
    "### **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f43bebe",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f2fae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "058a0c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'n_estimators': 300, 'min_samples_split': 15, 'min_samples_leaf': 5, 'max_features': 'log2', 'max_depth': 12, 'criterion': 'entropy', 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "param_range = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [7, 10,12], \n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_split': [10, 15, 20],\n",
    "    'min_samples_leaf': [5, 10, 15],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "rf_clf_rs = RandomizedSearchCV(rf_clf, param_range, n_iter=20, cv=cv_strategy, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "rf_clf_rs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", rf_clf_rs.best_params_)\n",
    "best_rf = rf_clf_rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cf52b343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.782\n",
      "Accuracy on train: 0.889\n"
     ]
    }
   ],
   "source": [
    "best_rf_proba = best_rf.predict_proba(X_val)[:,1]\n",
    "best_rf_pred = best_rf.predict(X_val)\n",
    "\n",
    "best_rf_pred_tr = best_rf.predict(X_train)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_val, best_rf_pred):.3f}\")\n",
    "\n",
    "print(f\"Accuracy on train: {accuracy_score(y_train, best_rf_pred_tr):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014fde3d",
   "metadata": {},
   "source": [
    "## **KNClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4c3d25fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa2949",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "365135ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'leaf_size': 20, 'metric': 'manhattan', 'n_neighbors': 30, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "param_set = {\n",
    "    'n_neighbors': [15, 20, 25, 30],\n",
    "    'weights': ['uniform'],\n",
    "    'leaf_size': [20, 30, 40],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "knn_clf_rs = GridSearchCV(knn_clf, param_set, cv=cv_strategy, scoring='accuracy', n_jobs=-1)\n",
    "knn_clf_rs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", knn_clf_rs.best_params_)\n",
    "best_knn = knn_clf_rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68071c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "opcional pôr \n",
    "\n",
    "\n",
    "results = knn_grid.cv_results_['mean_test_score']\n",
    "plt.plot(range(1, 40), results, marker='o')\n",
    "plt.xlabel('Number of Neighbors (K)')\n",
    "plt.ylabel('CV Accuracy')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3c32fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.776\n",
      "Accuracy on train: 0.771\n"
     ]
    }
   ],
   "source": [
    "best_knn_proba = best_knn.predict_proba(X_val)[:,1]\n",
    "best_knn_pred = best_knn.predict(X_val)\n",
    "\n",
    "best_knn_pred_tr = best_knn.predict(X_train)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_val, best_knn_pred):.3f}\")\n",
    "\n",
    "print(f\"Accuracy on train: {accuracy_score(y_train, best_knn_pred_tr):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d31939",
   "metadata": {},
   "source": [
    "## **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9eda6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_clf = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc2b053",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1d6fdbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tiago\\anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'penalty': 'l1', 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'], #L1 = lasso L2 = ridge\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "}\n",
    "\n",
    "logr_clf_rs = RandomizedSearchCV(logr_clf, param_grid, cv=cv_strategy, scoring='accuracy', n_jobs=-1)\n",
    "logr_clf_rs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", logr_clf_rs.best_params_)\n",
    "best_logr = logr_clf_rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "39dec9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.731\n"
     ]
    }
   ],
   "source": [
    "best_logr_proba = best_logr.predict_proba(X_val)[:,1]\n",
    "best_logr_pred = best_logr.predict(X_val)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_val, best_logr_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2061c108",
   "metadata": {},
   "source": [
    "## **Stacking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "92dd0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('rf', best_rf),\n",
    "    ('lgb', best_lgb),\n",
    "    ('knn', best_knn),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "406f46c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Individual Model Performance ---\n",
      "rf: 0.782\n",
      "[LightGBM] [Info] Number of positive: 2311, number of negative: 1328\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2221\n",
      "[LightGBM] [Info] Number of data points in the train set: 3639, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.635065 -> initscore=0.554006\n",
      "[LightGBM] [Info] Start training from score 0.554006\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "lgb: 0.764\n",
      "knn: 0.776\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Individual Model Performance ---\")\n",
    "for name, model in estimators:\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_val, model.predict(X_val))\n",
    "    print(f\"{name}: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4c284c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stc = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=cv_strategy, n_jobs=-1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "718721b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.783\n",
      "Accuracy on train: 0.865\n"
     ]
    }
   ],
   "source": [
    "stc_proba = stc.predict_proba(X_val)[:,1]\n",
    "stc_pred = stc.predict(X_val)\n",
    "stc_pred_tr = stc.predict(X_train)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_val, stc_pred):.3f}\")\n",
    "\n",
    "print(f\"Accuracy on train: {accuracy_score(y_train, stc_pred_tr):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "baf2c1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.778\n"
     ]
    }
   ],
   "source": [
    "stc_proba_test = stc.predict_proba(X_test)[:,1]\n",
    "stc_pred_test = stc.predict(X_test)\n",
    "stc_pred_test = stc.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, stc_pred_test):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a0b8d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Concatenate Train and Validation sets\n",
    "X_full = pd.concat([X_train, X_val, X_test], axis=0)\n",
    "y_full = pd.concat([y_train, y_val, y_test], axis=0)\n",
    "\n",
    "# 2. Re-train your BEST model on the full data\n",
    "# (Assuming 'best_rf' or 'stc' was your best estimator)\n",
    "final_model = stc  # or best_rf, best_lgb, etc.\n",
    "final_model.fit(X_full, y_full)\n",
    "\n",
    "# 3. Predict on the Kaggle data\n",
    "final_predictions = final_model.predict(X_predict)\n",
    "\n",
    "# 4. Save\n",
    "submission = pd.DataFrame({'id': id_predict, 'Quality_class': final_predictions})\n",
    "submission['Quality_class'] = submission['Quality_class'].map({0: 'KO', 1: 'OK'})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3433c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
