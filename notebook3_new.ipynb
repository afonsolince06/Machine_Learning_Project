{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb4923d",
   "metadata": {},
   "source": [
    "## **Notebook 3**\n",
    "## **Feature Management**\n",
    "\n",
    "### Introduction\n",
    "This notebook executes the core **Feature Engineering** phase by performing two critical operations: splitting the data and applying statistical transformations in a controlled, non-leaking sequence. Since we are **not using a Scikit-learn Pipeline**, all transformation steps (Imputation, Scaling, Encoding) must be manually fitted and applied.\n",
    "\n",
    "### Anti-Leakage Protocol: Split First\n",
    "1.  **Split Data First:** The structurally cleaned dataset is first divided into dedicated Training, Validation, and Test sets.\n",
    "2.  **Fit on Training Only:** All statistical transformers (`SimpleImputer`, `StandardScaler`, `OneHotEncoder`) are then **fitted exclusively on the Training set**.\n",
    "3.  **Transform All:** The parameters learned solely from the Training set are applied to transform the Training, Validation, and Test sets.\n",
    "\n",
    "This explicit, manual sequencing guarantees that no information from the Test or Validation set contaminates the Training process.\n",
    "\n",
    "### Objectives\n",
    "The objectives are:\n",
    "* **Data Splitting:** Divide the dataset into 60% Training, 20% Validation, and 20% Test sets using a stratified approach to maintain class balance (`OK`/`KO`) in all partitions.\n",
    "* **Imputation:** Manually fit and transform the data, filling missing values in numerical features using the median calculated only from the Training data.\n",
    "* **Scaling:** Manually fit and transform the numerical features using the `StandardScaler`, with its mean ($\\mu$) and standard deviation ($\\sigma$) calculated only from the Training data.\n",
    "* **Encoding:** Manually fit and transform the categorical feature (`origin`) using `OneHotEncoder` based only on the unique categories present in the Training data.\n",
    "* **Export:** Save the fully transformed data splits and the fitted transformer objects for direct use by the **Modelling (NB4)** and **Final Model (NB9)** notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61ff753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "# data partition\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold \n",
    "\n",
    "# imputação\n",
    "from sklearn.impute import SimpleImputer # <-- ESSENCIAL para imputação manual\n",
    "\n",
    "# scaling methods\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# encoding\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachieLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
